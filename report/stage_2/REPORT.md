# 2021-highload-dht

Курсовой проект в рамках обучающей программы "[Технополис](https://polis.mail.ru)" по дисциплине [Высоконагруженные вычисления](https://polis.mail.ru/curriculum/program/discipline/1257/).

## Этап 2. Многопоточность

Произведем профилирование имеющегося на данный момент сервера без внесения
каких-либо изменений в исходный код, чтобы определить проблемные места.

Для тестирования с помощью wrk2 были подобраны оптимальные параметры для тестируемого железа.
Их можно увидеть в запускаемом скрипте [wrk2.sh](../../profiling/wrk2.sh).
Все тесты будут проводиться на одних и тех же параметрах.

### Первоначальный анализ

 - #### [wrk2 для PUT-запросов](./profiling/wrk2_before.txt)

Видно, что для 90% пользователей запросы происходят в нормальное время до `2ms`,
а дальше время отклика сервера начинает значительный рост `(> 74ms)`.

- #### [async-profiler CPU для PUT-запросов](./profiling/profiler_cpu_before.html)

- #### [async-profiler LOCK для PUT-запросов](./profiling/profiler_lock_before.html)

На графике `LOCK для PUT` видно, что значительную часть времени простоя потоки
проводят на методе `LsmDAO.upsert` при вставке элементов. Попробуем это исправить.

### Оптимизация метода `upsert`

[53e0af88b2fd098b5fded4313ee2b60eb000ac75]()

Из метода `upsert` была удалена блокировка `synhronized`. Вместо неё реализована
неблокирующая запись в `MemTable`. \

Проведем измерения снова:

 - #### [wrk2 для PUT-запросов](./profiling/wrk2_upsert_put.txt)
 - #### [async-profiler CPU для PUT-запросов](./profiling/profiler_cpu_upsert_put.html)
 - #### [async-profiler LOCK для PUT-запросов](./profiling/profiler_lock_upsert_put.html)

Из результатов профилирования `LOCK` видно, что удалось избавиться от lock'а `LsmDAO.upsert`.
Он стал занимать `0.13%` против предыдущих `29.11%` от общего времени блокировки.

Что касается результатов `wrk2`, тот тут мы тоже в выигрыше: значительно соотношение
по квантилям не изменилось, однако средняя задержка упала почти на `1.5ms`.

### Оптимизация метода `flush`

[dde3059bddb53af228058be677bea3f02ffa30e0]()

В настоящий момент потоки-клиенты выполняют всю работу с записью на диск в случае 
переполнения MemTable самостоятельно. Это должно сильно задерживать клиентов,
потому что ответ они получают с задержкой.
Такую работу необходимо отдавать на выполнение другому ~~специально-обученному~~ потоку.
Однако тут нужно быть осторожным. Та ячейка памяти, которая отдана на сохранение,
не должна быть временно утеряна, она все ещё должна быть доступна для чтения клиентам.

Была проведена соответствующая оптимизация. `flush` выполняется в отдельном потоке.\
Для проверки доступности выгружаемой памяти был написан тест
`PersistenceTest.hugeRecordsWriteReadFlushTest`,
который пишет в память большие куски данных и сразу же пытается их прочесть.
Это как-то да и выявляет ситуацию, когда записанных данных в памяти нет.\
Кроме того, этот тест хорошо поможет, когда будем выполнять оптимизацию `GET`
далее по отчету.

Результаты измерений для PUT-запросов:

- #### [wrk2 для PUT-запросов](./profiling/wrk2_upsert_asyncflush_put.txt)
- #### [async-profiler CPU для PUT-запросов](./profiling/profiler_cpu_upsert_asyncflush_put.html)
- #### [async-profiler LOCK для PUT-запросов](./profiling/profiler_lock_upsert_asyncflush_put.html)

Результаты не могут не радовать: `99.90%` всех запросов выполняются до `5ms`.
Однако есть куда стремиться. На лекциях говорили, что добиться
можно и `99.99%` и даже `99.999%`. Будем над этим работать, но не обязательно сейчас.

Посмотрим, что происходит с GET-запросами, ведь на методе `LsmDAO.range` как 
висел `syncronized`, так он и висит.

Результаты измерений для GET-запросов:

- #### [wrk2 для GET-запросов](./profiling/wrk2_upsert_asyncflush_get.txt)
- #### [async-profiler CPU для GET-запросов](./profiling/profiler_cpu_upsert_asyncflush_get.html)
- #### [async-profiler LOCK для GET-запросов](./profiling/profiler_lock_upsert_asyncflush_get.html)

Результаты не очень. Несмотря на то, что сервер не сумел сдержать заданный интервал
в 70000 запросов в секунду, выводы сделать можно и есть с чем сравнить в будущем.
Много CPU занимает тот самый упомянутый `range` (`77.93%` времени).
На графике `LOCK` тоже этот же самый метод занимает `100%` времени.
Что оптимизировать предельно ясно.

### Оптимизация метода `range`

Сразу стоит оговориться про написанный тест `hugeRecordsWriteReadFlushTest`.
Если просто убрать блок `synchronized` у метода `range`, все тесты будут пройдены.
Однако разработанный тест завалится, что есть хорошо. Потому что на самом деле
данные при чтении и записи не согласованы.

Была проведена оптимизация метода `range` путем разделения доступа данным к
потоку-писателю и читателям, благодаря чему убрано узкое место для читателей
(они могут читать одновременно), и не нарушается консистентность данных
при чтении/записи.

Результаты измерений для PUT-запросов:

- #### [wrk2 для PUT-запросов](./profiling/wrk2_range_put.txt)
- #### [async-profiler CPU для PUT-запросов](./profiling/profiler_cpu_range_put.html)
- #### [async-profiler LOCK для PUT-запросов](./profiling/profiler_lock_range_put.html)
- #### [async-profiler ALLOC для PUT-запросов](./profiling/profiler_alloc_range_put.html)

Результаты измерений для GET-запросов:

- #### [wrk2 для GET-запросов](./profiling/wrk2_range_get.txt)
- #### [async-profiler CPU для GET-запросов](./profiling/profiler_cpu_range_get.html)
- #### [async-profiler LOCK для GET-запросов](./profiling/profiler_lock_range_get.html)
- #### [async-profiler ALLOC для GET-запросов](./profiling/profiler_alloc_range_get.html)

Исходя из данных `wrk2`, мы видим заметный прирост в производительности
для GET-запросов. Для PUT-запросов изменений в производительности нет.
Тест запись-чтение (`hugeRecordsWriteReadFlushTest`) успешно пройден.

Тест GET-запросами теперь выдерживает заданный интервал в 70000 запросов
в секунду.

В результате профилирования `lock` для GET-запросов видим ещё одно подтверждение,
что блокировка, которая ранее занимала `100%`, устранена. Основное время занимает 
парковка потоков `thread executor`.